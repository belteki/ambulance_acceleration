{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alt text](./Cerny_logo_1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The effect of ambulance acceleration on mechanical ventilation during neonatal transport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Author: Dr Gusztav Belteki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries and set options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from scipy import stats\n",
    "from pandas import Series, DataFrame\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('classic')\n",
    "matplotlib.rcParams['figure.facecolor'] = 'w'\n",
    "matplotlib.rcParams['date.autoformatter.second'] = '%H:%M:%S.%f'\n",
    "\n",
    "pd.set_option('display.max_rows', 250)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python version: {}\".format(sys.version))\n",
    "print(\"pandas version: {}\".format(pd.__version__))\n",
    "print(\"matplotlib version: {}\".format(matplotlib.__version__))\n",
    "print(\"NumPy version: {}\".format(np.__version__))\n",
    "print(\"SciPy version: {}\".format(sp.__version__))\n",
    "print(\"IPython version: {}\".format(IPython.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. List and set the working directory and the directory to write out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic of the Notebook which will also be the name of the subfolder containing results\n",
    "TOPIC = 'accelerometer_ventilated'\n",
    "\n",
    "# Name of the external hard drive\n",
    "DRIVE = 'GUSZTI'\n",
    "\n",
    "# Directory containing clinical and blood gas data\n",
    "CWD = os.path.join('/Users', 'guszti', 'ventilation_fabian')\n",
    "\n",
    "DIR_WRITE = os.path.join(CWD, 'Analyses', TOPIC)\n",
    "\n",
    "DATA_DUMP = os.path.join('/Volumes', DRIVE, 'data_dump/', 'fabian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_WRITE, DATA_DUMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import ventilator and accelerometer data from pickle archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/%s.pickle' % (DATA_DUMP, 'data_pars_measurements_ventilated_accelero'), 'rb') as handle:\n",
    "    data_pars_measurements = pickle.load(handle)\n",
    "with open('%s/%s.pickle' % (DATA_DUMP, 'data_pars_settings_ventilated_accelero'), 'rb') as handle:\n",
    "    data_pars_settings = pickle.load(handle)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('%s/%s.pickle' % (DATA_DUMP, 'accelero_ventilated_1'), 'rb') as handle:\n",
    "    accelero_ventilated = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(data_pars_measurements), len(data_pars_settings), len(accelero_ventilated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate recording durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recording_durations_vent = {}\n",
    "\n",
    "for recording in data_pars_measurements:\n",
    "    if len(data_pars_measurements[recording]) > 0:\n",
    "         recording_durations_vent[recording] = data_pars_measurements[recording].index[-1] - \\\n",
    "            data_pars_measurements[recording].index[0]\n",
    "    \n",
    "recording_durations_vent = Series(recording_durations_vent)\n",
    "recording_durations_vent;\n",
    "\n",
    "recording_durations_accel = {}\n",
    "\n",
    "for recording in accelero_ventilated:\n",
    "    if len(accelero_ventilated[recording]) > 0:\n",
    "         recording_durations_accel[recording] = accelero_ventilated[recording].index[-1] - \\\n",
    "            accelero_ventilated[recording].index[0]\n",
    "    \n",
    "recording_durations_accel = Series(recording_durations_accel)\n",
    "recording_durations_accel;\n",
    "\n",
    "# Combine recording_durations\n",
    "recording_durations = DataFrame([recording_durations_accel, recording_durations_vent]).T\n",
    "recording_durations.columns = ['accel', 'vent']\n",
    "\n",
    "#recording_durations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_durations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_durations.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recording_durations.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Remove those recordings that are less that 15 minutes long\n",
    "\n",
    "Some journey were done within Budapest and hence are very short.\n",
    "This is not the net time the ambulance was moving. It includes time when the team was moving the trolley within the hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recording_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(recording_durations[recording_durations['vent'] >= pd.to_timedelta('15M')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recording_durations[recording_durations['vent'] < pd.to_timedelta('15M')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(recording_durations[recording_durations['accel'] >= pd.to_timedelta('15M')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recording_durations[recording_durations['accel'] < pd.to_timedelta('15M')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_to_keep = sorted(recording_durations[recording_durations['accel'] > \n",
    "                        pd.to_timedelta('15M')]['accel'].to_dict())\n",
    "matches_to_keep;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelero_ventilated = {key: value for key, value in accelero_ventilated.items() \n",
    "                       if key in matches_to_keep}\n",
    "\n",
    "data_pars_measurements = {key: value for key, value in data_pars_measurements.items() \n",
    "                       if key in matches_to_keep}\n",
    "\n",
    "data_pars_settings = {key: value for key, value in data_pars_settings.items() \n",
    "                       if key in matches_to_keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(data_pars_measurements), len(data_pars_settings), len(accelero_ventilated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Import data about times when the team left the Unit and when the ambulance was moving\n",
    "\n",
    "This table was generated by the clinicians after manually reviewing the case records\n",
    "\n",
    "The table also contains the ID of the ambulance. Suspension in different ambulances may be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_movement_times = pd.read_excel(os.path.join(CWD, 'amb_movement_times_extended.xlsx'), index_col = 'Recording')\n",
    "amb_movement_times = amb_movement_times[['Time ambulance departed from referring hospital',\n",
    "                                         'Time ambulance arrived at destination hospital']]\n",
    "amb_movement_times['Journey time'] = amb_movement_times['Time ambulance arrived at destination hospital'] - \\\n",
    "    amb_movement_times['Time ambulance departed from referring hospital']\n",
    "amb_movement_times.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_movement_times.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In some cases the ambulance journey was very short, less than 10 minutes\n",
    "len(amb_movement_times[amb_movement_times['Journey time'] < pd.to_timedelta('10M')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Keep only periods from the departure of the ambulance to the arrival of the ambulance\n",
    "\n",
    "There could still be periods when the ambulance stopped during the transfer, these need to be reviewed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rec_list = list(accelero_ventilated.keys())\n",
    "\n",
    "for recording in rec_list:\n",
    "    if recording[1] in amb_movement_times.index:\n",
    "    \n",
    "        a = amb_movement_times.loc[recording[1]]['Time ambulance departed from referring hospital']\n",
    "        b = amb_movement_times.loc[recording[1]]['Time ambulance arrived at destination hospital']\n",
    "    \n",
    "        accelero_ventilated[recording] = accelero_ventilated[recording][a : b]\n",
    "        data_pars_measurements[recording] = data_pars_measurements[recording][a : b]\n",
    "        data_pars_settings[recording] = data_pars_settings[recording][a : b]\n",
    "    \n",
    "    else: \n",
    "        del accelero_ventilated[recording]\n",
    "        del data_pars_measurements[recording]\n",
    "        del data_pars_settings[recording]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_pars_measurements), len(data_pars_settings), len(accelero_ventilated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Calculate recording durations of the trimmed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_durations_vent = {}\n",
    "\n",
    "for recording in data_pars_measurements:\n",
    "    if len(data_pars_measurements[recording]) > 0:\n",
    "         recording_durations_vent[recording] = data_pars_measurements[recording].index[-1] - \\\n",
    "            data_pars_measurements[recording].index[0]\n",
    "    \n",
    "recording_durations_vent = Series(recording_durations_vent)\n",
    "recording_durations_vent;\n",
    "\n",
    "recording_durations_accel = {}\n",
    "\n",
    "for recording in accelero_ventilated:\n",
    "    if len(accelero_ventilated[recording]) > 0:\n",
    "         recording_durations_accel[recording] = accelero_ventilated[recording].index[-1] - \\\n",
    "            accelero_ventilated[recording].index[0]\n",
    "    \n",
    "recording_durations_accel = Series(recording_durations_accel)\n",
    "recording_durations_accel;\n",
    "\n",
    "# Combine recording_durations\n",
    "recording_durations = DataFrame([recording_durations_accel, recording_durations_vent]).T\n",
    "recording_durations.columns = ['accel', 'vent']\n",
    "\n",
    "#recording_durations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recording_durations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recording_durations.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Remove those recordings when the net moving time was less than 10 minutes\n",
    "\n",
    "This still includes potential stops during the journey, they need to be reviewed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(recording_durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(recording_durations[recording_durations['accel'] > pd.to_timedelta('10M')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_to_keep = sorted(recording_durations[recording_durations['accel'] > \n",
    "                        pd.to_timedelta('10M')]['accel'].to_dict())\n",
    "matches_to_keep;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelero_ventilated = {key: value for key, value in accelero_ventilated.items() \n",
    "                       if key in matches_to_keep}\n",
    "\n",
    "data_pars_measurements = {key: value for key, value in data_pars_measurements.items() \n",
    "                       if key in matches_to_keep}\n",
    "\n",
    "data_pars_settings = {key: value for key, value in data_pars_settings.items() \n",
    "                       if key in matches_to_keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(data_pars_measurements), len(data_pars_settings), len(accelero_ventilated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Calculate final recording durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording_durations_vent = {}\n",
    "\n",
    "for recording in data_pars_measurements:\n",
    "    if len(data_pars_measurements[recording]) > 0:\n",
    "         recording_durations_vent[recording] = data_pars_measurements[recording].index[-1] - \\\n",
    "                                     data_pars_measurements[recording].index[0]\n",
    "    \n",
    "recording_durations_vent = Series(recording_durations_vent)\n",
    "recording_durations_vent;\n",
    "\n",
    "recording_durations_accel = {}\n",
    "\n",
    "for recording in accelero_ventilated:\n",
    "    if len(accelero_ventilated[recording]) > 0:\n",
    "         recording_durations_accel[recording] = accelero_ventilated[recording].index[-1] - \\\n",
    "                                                  accelero_ventilated[recording].index[0]\n",
    "    \n",
    "recording_durations_accel = Series(recording_durations_accel)\n",
    "recording_durations_accel;\n",
    "\n",
    "# Combine recording_durations\n",
    "recording_durations = DataFrame([recording_durations_accel, recording_durations_vent]).T\n",
    "recording_durations.columns = ['accel', 'vent']\n",
    "\n",
    "#recording_durations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recording_durations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recording_durations.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Subtract gravity from vertical (`Z`) acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rec in accelero_ventilated:\n",
    "    accelero_ventilated[rec]['Z'] = accelero_ventilated[rec]['Z'] - 9.81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Pass accelerometer data through a low pass filter and a high pass filter\n",
    "\n",
    "Low pass filter isolates true acceleration, high pass filter isolates vibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# third order Butterworth filter\n",
    "# Wn is the critical frequency, as fraction of the Nyquist frequency\n",
    "# As sampling rate was 100 Hz, Nyquist frequency is 50Hz critical frequency is 0.5 Hz\n",
    "b, a = scipy.signal.butter(N=3, Wn=0.01, btype='lowpass')\n",
    "c, d = scipy.signal.butter(N=3, Wn=0.01, btype='highpass')\n",
    "\n",
    "accelero_ventilated_low_pass = {}\n",
    "accelero_ventilated_high_pass = {}\n",
    "\n",
    "for recording, data in accelero_ventilated.items():\n",
    "    #print(recording)\n",
    "    data_low_pass = {}\n",
    "    data_high_pass = {}\n",
    "    for item in ['X', 'Y', 'Z']:\n",
    "        data_low_pass[item] =  scipy.signal.filtfilt(b, a, data[item])\n",
    "        data_high_pass[item] =  scipy.signal.filtfilt(c, d, data[item])\n",
    "    \n",
    "    accelero_ventilated_low_pass[recording] = DataFrame(data_low_pass)\n",
    "    accelero_ventilated_low_pass[recording].index = accelero_ventilated[recording].index\n",
    "    accelero_ventilated_high_pass[recording] = DataFrame(data_high_pass)\n",
    "    accelero_ventilated_high_pass[recording].index = accelero_ventilated[recording].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Calculate absolute values of the accelerations in each direction as well as the length (Euclidean norm, also known as L2 norm) of the acceleration vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for rec in accelero_ventilated:\n",
    "    \n",
    "    # absolute value of the vectors\n",
    "    accelero_ventilated[rec]['X_abs'] = np.abs(accelero_ventilated[rec]['X'])\n",
    "    accelero_ventilated[rec]['Y_abs'] = np.abs(accelero_ventilated[rec]['Y'])\n",
    "    accelero_ventilated[rec]['Z_abs'] = np.abs(accelero_ventilated[rec]['Z'])\n",
    "    \n",
    "    # Euclidean norm of the acceleration vector\n",
    "    accelero_ventilated[rec]['length'] = np.sqrt(accelero_ventilated[rec]['X'] ** 2 + \n",
    "        accelero_ventilated[rec]['Y'] ** 2 + accelero_ventilated[rec]['Z'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for rec in accelero_ventilated_low_pass:\n",
    "    \n",
    "    # absolute value of the vectors\n",
    "    accelero_ventilated_low_pass[rec]['X_abs'] = np.abs(accelero_ventilated_low_pass[rec]['X'])\n",
    "    accelero_ventilated_low_pass[rec]['Y_abs'] = np.abs(accelero_ventilated_low_pass[rec]['Y'])\n",
    "    accelero_ventilated_low_pass[rec]['Z_abs'] = np.abs(accelero_ventilated_low_pass[rec]['Z'])\n",
    "    \n",
    "    # Euclidean norm of the acceleration vector\n",
    "    accelero_ventilated_low_pass[rec]['length'] = np.sqrt(accelero_ventilated_low_pass[rec]['X'] ** 2 + \n",
    "        accelero_ventilated_low_pass[rec]['Y'] ** 2 + accelero_ventilated_low_pass[rec]['Z'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for rec in accelero_ventilated_high_pass:\n",
    "    \n",
    "    # absolute value of the vectors\n",
    "    accelero_ventilated_high_pass[rec]['X_abs'] = np.abs(accelero_ventilated_high_pass[rec]['X'])\n",
    "    accelero_ventilated_high_pass[rec]['Y_abs'] = np.abs(accelero_ventilated_high_pass[rec]['Y'])\n",
    "    accelero_ventilated_high_pass[rec]['Z_abs'] = np.abs(accelero_ventilated_high_pass[rec]['Z'])\n",
    "    \n",
    "    # Euclidean norm of the acceleration vector\n",
    "    accelero_ventilated_high_pass[rec]['length'] = np.sqrt(accelero_ventilated_high_pass[rec]['X'] ** 2 + \n",
    "        accelero_ventilated_high_pass[rec]['Y'] ** 2 + accelero_ventilated_high_pass[rec]['Z'] ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Calculate the directional X and Y acceleration (separate positive and negative values in these directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for rec in accelero_ventilated_low_pass:\n",
    "    \n",
    "    # generate columns with only the positive or only the negative values of X and Y acceleleration considered\n",
    "    accelero_ventilated_low_pass[rec]['X_pos'] = accelero_ventilated_low_pass[rec]['X'].apply(lambda x: max(x, 0))\n",
    "    accelero_ventilated_low_pass[rec]['X_neg'] = accelero_ventilated_low_pass[rec]['X'].apply(lambda x: min(x, 0))\n",
    "    accelero_ventilated_low_pass[rec]['Y_pos'] = accelero_ventilated_low_pass[rec]['Y'].apply(lambda x: max(x, 0))\n",
    "    accelero_ventilated_low_pass[rec]['Y_neg'] = accelero_ventilated_low_pass[rec]['Y'].apply(lambda x: min(x, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Resample accelerometer data\n",
    "\n",
    "Accelerometer data are not following a normal distribution. Use median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "accelero_ventilated_1min_median = {}\n",
    "\n",
    "columns_to_keep = ['X_abs', 'Y_abs', 'Z_abs', 'length']\n",
    "\n",
    "for rec in sorted(accelero_ventilated.keys()):\n",
    "    accelero_ventilated_1min_median[rec] = accelero_ventilated[rec].resample('1Min').median()[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "accelero_ventilated_low_pass_1min_median = {}\n",
    "\n",
    "columns_to_keep_1 = ['X_abs', 'Y_abs', 'Z_abs', 'length']\n",
    "columns_to_keep_2 = ['X_pos', 'X_neg', 'Y_pos', 'Y_neg']\n",
    "\n",
    "for rec in sorted(accelero_ventilated_low_pass.keys()):\n",
    "    \n",
    "    accelero_ventilated_low_pass_1min_median[rec] = \\\n",
    "        accelero_ventilated_low_pass[rec].resample('1Min').median()[columns_to_keep_1]\n",
    "    \n",
    "    # Az 0 data points here represent negative acceleration values converted to zeros\n",
    "    # they should be excluded from median\n",
    "    for par in columns_to_keep_2:\n",
    "        data = accelero_ventilated_low_pass[rec][par][accelero_ventilated_low_pass[rec][par] != 0]\n",
    "        accelero_ventilated_low_pass_1min_median[rec][par] = data.resample('1Min').median()\n",
    "    \n",
    "    accelero_ventilated_low_pass_1min_median[rec].rename(lambda x: x + '_median', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "accelero_ventilated_high_pass_1min_median = {}\n",
    "\n",
    "columns_to_keep = ['X_abs', 'Y_abs', 'Z_abs', 'length']\n",
    "\n",
    "for rec in sorted(accelero_ventilated_low_pass.keys()):\n",
    "    \n",
    "    accelero_ventilated_high_pass_1min_median[rec] = \\\n",
    "        accelero_ventilated_high_pass[rec].resample('1Min').median()[columns_to_keep]\n",
    "    \n",
    "    accelero_ventilated_high_pass_1min_median[rec].rename(lambda x: x + '_median', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Keep only relevant columns of ventilator data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in data_pars_measurements:\n",
    "    \n",
    "    # MV and VT are not meaningful without weight correction\n",
    "    data_pars_measurements[recording].drop(['MV', 'VTemand', 'VTimand'], axis = 1, inplace = True)\n",
    "    \n",
    "    if 'VTemand_resp' in data_pars_measurements[recording]:\n",
    "        # Only SIMV recordings have these columns\n",
    "        data_pars_measurements[recording].drop(['VTemand_resp', 'VTespon_pat'], axis = 1, inplace = True)\n",
    "\n",
    "    # Remove empty rows\n",
    "    data_pars_measurements[recording].dropna(how = 'all', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Add relevant columns from ventilator settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting_to_keep_1 = ['PIP_set', 'PEEP_set', 'FiO2_set', 'Ti_set', 'Te_set', 'RR_set', 'VG_set_kg', \n",
    "                     'IE_I_set', 'IE_E_set', 'Trigger_sens_set', 'Ventilator_mode']\n",
    "\n",
    "setting_to_keep_1B = ['PIP_set', 'PEEP_set', 'FiO2_set', 'Ti_set', 'Te_set', 'RR_set', \n",
    "                      'IE_I_set', 'IE_E_set', 'Trigger_sens_set', 'Ventilator_mode']\n",
    "\n",
    "data_pars_combined = {}\n",
    "\n",
    "for recording in data_pars_measurements:\n",
    "    # Recordings containing volume guarantee ventilation\n",
    "    if 'VG_set_kg' in data_pars_settings[recording]:\n",
    "        data_pars_combined[recording] = pd.merge(data_pars_measurements[recording], \n",
    "                data_pars_settings[recording][setting_to_keep_1], left_index= True, right_index = True)\n",
    "    \n",
    "    # Recordings without VG\n",
    "    else: \n",
    "        data_pars_combined[recording] = pd.merge(data_pars_measurements[recording], \n",
    "                data_pars_settings[recording][setting_to_keep_1B], left_index= True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Add derived parameters to ventilator data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VTdiff: the difference between the sew and the actual VT during VG ventilation\n",
    "- Pdiff_VG: the difference between Pmax and PIP during VG ventilation\n",
    "- Pdiff_noVG: the difference between the set and the actual PIP during pressure limited ventilation\n",
    "- RR_diff: the difference between actual RR and the set RR (this will only work during SIPPV as during SIMV the spontaneous breaths' RR is not reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in data_pars_combined:\n",
    "    if 'VG_set_kg' in data_pars_combined[recording]:\n",
    "        data_pars_combined[recording]['VTdiff'] = (data_pars_combined[recording]['VTemand_kg'] - \n",
    "                                                   data_pars_combined[recording]['VG_set_kg'])\n",
    "        data_pars_combined[recording]['VTdiff_abs'] = np.abs(data_pars_combined[recording]['VTemand_kg'] - \n",
    "                                                             data_pars_combined[recording]['VG_set_kg'])\n",
    "        data_pars_combined[recording]['Pdiff_VG'] = (data_pars_combined[recording]['PIP_set'] - \n",
    "                                                     data_pars_combined[recording]['PIP'])\n",
    "        data_pars_combined[recording]['Pdiff_VG_abs'] = np.abs(data_pars_combined[recording]['PIP_set'] - \n",
    "                                                               data_pars_combined[recording]['PIP'])\n",
    "                                                     \n",
    "    else:\n",
    "        data_pars_combined[recording]['Pdiff_noVG'] = (data_pars_combined[recording]['PIP_set'] - \n",
    "                                                       data_pars_combined[recording]['PIP'])\n",
    "        data_pars_combined[recording]['Pdiff_noVG_abs'] = np.abs(data_pars_combined[recording]['PIP_set'] - \n",
    "                                                                 data_pars_combined[recording]['PIP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording in data_pars_combined:\n",
    "    if 'RR' in data_pars_combined[recording] and 'RR_set' in data_pars_combined[recording]:\n",
    "        data_pars_combined[recording]['RRdiff'] = (data_pars_combined[recording]['RR'] - \n",
    "                                                   data_pars_combined[recording]['RR_set'])\n",
    "        data_pars_combined[recording]['RRdiff_abs'] = np.abs(data_pars_combined[recording]['RR'] - \n",
    "                                                             data_pars_combined[recording]['RR_set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Generate DataFrames with the ventilator modes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventilator_mode = {}\n",
    "\n",
    "for recording in data_pars_settings:\n",
    "    ventilator_mode[recording] = data_pars_settings[recording][['Ventilator_mode']] \n",
    "\n",
    "ventilator_mode[('default__393.txt', 'AL000628')];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Resample ventilator data\n",
    "\n",
    "Some ventilator parameters follow normal distribution. For them, use mean and SD for aggregation.\n",
    "Other parameters do not follow normal distribution. For them, calculate median and IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_pars_combined_1min_mean = {}\n",
    "data_pars_combined_1min_std = {}\n",
    "data_pars_combined_1min_median = {}\n",
    "\n",
    "for recording in data_pars_combined:\n",
    "    \n",
    "    data_pars_combined_1min_mean[recording] = data_pars_combined[recording].resample('1Min').mean()\n",
    "    data_pars_combined_1min_std[recording] = data_pars_combined[recording].resample('1Min').std()\n",
    "    data_pars_combined_1min_median[recording] = data_pars_combined[recording].resample('1Min').median()\n",
    "    data_pars_combined_1min_median[recording].rename(mapper = lambda x: x + '_median', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Combine aggregated accelerometer and ventilator data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ventilated_1min  = {}\n",
    "\n",
    "for recording in accelero_ventilated:\n",
    "    \n",
    "    combined_ventilated_1min[recording] = pd.merge(data_pars_combined_1min_mean[recording], \n",
    "        data_pars_combined_1min_std[recording],\n",
    "        how = 'inner', left_index = True, right_index= True, suffixes = ['_mean', '_sd'])\n",
    "    \n",
    "    combined_ventilated_1min[recording] = pd.merge(combined_ventilated_1min[recording], \n",
    "        data_pars_combined_1min_median[recording],\n",
    "        how = 'inner', left_index = True, right_index= True,)\n",
    "    \n",
    "    combined_ventilated_1min[recording] = pd.merge(combined_ventilated_1min[recording], \n",
    "        accelero_ventilated_low_pass_1min_median[recording],\n",
    "        how = 'inner', left_index = True, right_index= True)\n",
    "    \n",
    "    combined_ventilated_1min[recording] = pd.merge(combined_ventilated_1min[recording], \n",
    "        accelero_ventilated_high_pass_1min_median[recording], \n",
    "        how = 'inner', left_index = True, right_index= True, suffixes = ['_low_pass', '_high_pass'])\n",
    "    \n",
    "    combined_ventilated_1min[recording].dropna(how = 'all', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Export aggregated accelerometer data as pickle archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/%s.pickle' % (DATA_DUMP, 'data_pars_combined'), 'wb') as handle:\n",
    "    pickle.dump(data_pars_combined, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('%s/%s.pickle' % (DATA_DUMP, 'combined_ventilated_1min'), 'wb') as handle:\n",
    "    pickle.dump(combined_ventilated_1min, handle, protocol=pickle.HIGHEST_PROTOCOL)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('%s/%s.pickle' % (DATA_DUMP, 'accelero_ventilated_2'), 'wb') as handle:\n",
    "    pickle.dump(accelero_ventilated, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('%s/%s.pickle' % (DATA_DUMP, 'accelero_ventilated_low_pass'), 'wb') as handle:\n",
    "    pickle.dump(accelero_ventilated_low_pass, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('%s/%s.pickle' % (DATA_DUMP, 'accelero_ventilated_high_pass'), 'wb') as handle:\n",
    "    pickle.dump(accelero_ventilated_high_pass, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/%s.pickle' % (DATA_DUMP, 'ventilator_mode'), 'wb') as handle:\n",
    "    pickle.dump(ventilator_mode, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
